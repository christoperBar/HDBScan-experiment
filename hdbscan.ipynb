{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72f0fbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepface import DeepFace\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import umap\n",
    "import hdbscan\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from PIL import Image\n",
    "from collections import defaultdict, Counter\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import hashlib\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74fcc2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_cache_status(cache_dir=\"coba_cache\"):\n",
    "    \"\"\"\n",
    "    Display detailed information about the current cache status\n",
    "    \"\"\"\n",
    "    print(\"üîç CACHE STATUS REPORT\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    faces_path = os.path.join(cache_dir, \"faces.npy\")\n",
    "    metadata_path = os.path.join(cache_dir, \"metadata.pkl\")\n",
    "    cache_info_path = os.path.join(cache_dir, \"cache_info.pkl\")\n",
    "    \n",
    "    if not os.path.exists(cache_dir):\n",
    "        print(\"‚ùå Cache directory does not exist\")\n",
    "        return\n",
    "    \n",
    "    # Check core cache files\n",
    "    has_faces = os.path.exists(faces_path)\n",
    "    has_metadata = os.path.exists(metadata_path)\n",
    "    has_cache_info = os.path.exists(cache_info_path)\n",
    "    \n",
    "    print(f\"üìÅ Cache Directory: {cache_dir}\")\n",
    "    print(f\"üìÑ Faces file: {'‚úÖ Exists' if has_faces else '‚ùå Missing'}\")\n",
    "    print(f\"üìÑ Metadata file: {'‚úÖ Exists' if has_metadata else '‚ùå Missing'}\")\n",
    "    print(f\"üìÑ Cache info: {'‚úÖ Exists' if has_cache_info else '‚ùå Missing'}\")\n",
    "    \n",
    "    if has_faces and has_metadata:\n",
    "        try:\n",
    "            # Load and show cache contents\n",
    "            faces = np.load(faces_path)\n",
    "            with open(metadata_path, \"rb\") as f:\n",
    "                metadata = pickle.load(f)\n",
    "            \n",
    "            print(f\"\\nüìä CACHED DATA:\")\n",
    "            print(f\"   ‚Ä¢ Face embeddings: {len(faces)}\")\n",
    "            \n",
    "            # Only show dimensions if faces exist\n",
    "            if len(faces) > 0:\n",
    "                print(f\"   ‚Ä¢ Face dimensions: {faces.shape[1]}D\")\n",
    "            else:\n",
    "                print(f\"   ‚Ä¢ Face dimensions: N/A (empty cache)\")\n",
    "                \n",
    "            print(f\"   ‚Ä¢ Metadata entries: {len(metadata)}\")\n",
    "            print(f\"   ‚Ä¢ File sizes: faces={os.path.getsize(faces_path)/1024/1024:.1f}MB, \"\n",
    "                  f\"metadata={os.path.getsize(metadata_path)/1024/1024:.1f}MB\")\n",
    "            \n",
    "            # Show cache info if available\n",
    "            if has_cache_info:\n",
    "                try:\n",
    "                    with open(cache_info_path, \"rb\") as f:\n",
    "                        cache_info = pickle.load(f)\n",
    "                    \n",
    "                    print(f\"\\nüìÖ CACHE DETAILS:\")\n",
    "                    print(f\"   ‚Ä¢ Created: {cache_info.get('created_time', 'Unknown')}\")\n",
    "                    print(f\"   ‚Ä¢ Source files: {cache_info.get('folder_stats', {}).get('total_files', 'Unknown')}\")\n",
    "                    print(f\"   ‚Ä¢ Extraction params: {cache_info.get('extraction_params', {})}\")\n",
    "                    \n",
    "                except:\n",
    "                    print(\"\\n‚ö†Ô∏è  Cache info file corrupted\")\n",
    "            \n",
    "            if len(faces) > 0:\n",
    "                print(f\"\\n‚úÖ Cache is ready to use!\")\n",
    "            else:\n",
    "                print(f\"\\n‚ö†Ô∏è  Cache is empty - no faces found\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\n‚ùå Error reading cache: {e}\")\n",
    "    else:\n",
    "        print(f\"\\n‚ùå Cache is incomplete - missing required files\")\n",
    "    \n",
    "    print(\"\\nüí° To force re-extraction, set FORCE_REEXTRACT=True in the script\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd6ce2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_folder_stats(folder_path):\n",
    "    \"\"\"\n",
    "    Get statistics about folder structure for cache validation\n",
    "    \"\"\"\n",
    "    stats = {\n",
    "        'total_files': 0,\n",
    "        'total_size': 0,\n",
    "        'folder_structure': {},\n",
    "        'last_modified': 0\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        for root, dirs, files in os.walk(folder_path):\n",
    "            for file in files:\n",
    "                if file.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp')):\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    try:\n",
    "                        file_stat = os.stat(file_path)\n",
    "                        stats['total_files'] += 1\n",
    "                        stats['total_size'] += file_stat.st_size\n",
    "                        stats['last_modified'] = max(stats['last_modified'], file_stat.st_mtime)\n",
    "                        \n",
    "                        # Track folder structure\n",
    "                        rel_path = os.path.relpath(root, folder_path)\n",
    "                        if rel_path not in stats['folder_structure']:\n",
    "                            stats['folder_structure'][rel_path] = 0\n",
    "                        stats['folder_structure'][rel_path] += 1\n",
    "                    except:\n",
    "                        continue\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4c6ed31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_faces_from_event_folder(event_folder_path, cache_dir=\"coba_cache\", cropped_dir=\"cropted_faces\", min_face_size=27, force_reextract=False):\n",
    "    \"\"\"\n",
    "    Extract faces from event folder with robust caching system\n",
    "    \n",
    "    Args:\n",
    "        event_folder_path: Path to folder containing images (single dir or nested albums)\n",
    "        cache_dir: Directory to store cached face embeddings  \n",
    "        cropped_dir: Directory to store cropped face images\n",
    "        min_face_size: Minimum face size to include\n",
    "        force_reextract: If True, ignore cache and re-extract all faces\n",
    "    \"\"\"\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    os.makedirs(cropped_dir, exist_ok=True)\n",
    "\n",
    "    faces_path = os.path.join(cache_dir, \"faces.npy\")\n",
    "    metadata_path = os.path.join(cache_dir, \"metadata.pkl\")\n",
    "    cache_info_path = os.path.join(cache_dir, \"cache_info.pkl\")\n",
    "\n",
    "    # Check if we should use cached data\n",
    "    use_cache = not force_reextract and os.path.exists(faces_path) and os.path.exists(metadata_path)\n",
    "    \n",
    "    if use_cache:\n",
    "        try:\n",
    "            # Validate cache integrity\n",
    "            print(\"üîç Checking cached face data...\")\n",
    "            \n",
    "            # Load cache info for validation\n",
    "            cache_info = {}\n",
    "            if os.path.exists(cache_info_path):\n",
    "                try:\n",
    "                    with open(cache_info_path, \"rb\") as f:\n",
    "                        cache_info = pickle.load(f)\n",
    "                except:\n",
    "                    print(\"‚ö†Ô∏è  Cache info corrupted, will re-extract\")\n",
    "                    use_cache = False\n",
    "            \n",
    "            if use_cache:\n",
    "                # Check if source folder structure changed\n",
    "                current_stats = get_folder_stats(event_folder_path)\n",
    "                cached_stats = cache_info.get('folder_stats', {})\n",
    "                \n",
    "                if current_stats != cached_stats:\n",
    "                    print(\"‚ö†Ô∏è  Source images changed, cache invalid - will re-extract\")\n",
    "                    use_cache = False\n",
    "                else:\n",
    "                    # Load cached data\n",
    "                    print(\"‚úÖ Loading cached face data...\")\n",
    "                    faces = np.load(faces_path)\n",
    "                    with open(metadata_path, \"rb\") as f:\n",
    "                        metadata = pickle.load(f)\n",
    "                    \n",
    "                    print(f\"üìÇ Loaded {len(faces)} cached face embeddings\")\n",
    "                    print(f\"üìÇ Loaded {len(metadata)} cached metadata entries\")\n",
    "                    print(f\"üìÇ Cache created: {cache_info.get('created_time', 'Unknown')}\")\n",
    "                    \n",
    "                    return faces, metadata\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading cache: {e}\")\n",
    "            print(\"üîÑ Will re-extract faces...\")\n",
    "            use_cache = False\n",
    "\n",
    "    faces = []\n",
    "    metadata = []\n",
    "\n",
    "    event_id = os.path.basename(event_folder_path)\n",
    "\n",
    "    print(f\"Processing event folder: {event_folder_path}\")\n",
    "    \n",
    "    # Check if this is a single directory or nested structure\n",
    "    if not os.path.exists(event_folder_path):\n",
    "        print(f\"‚ùå Error: Source folder '{event_folder_path}' does not exist!\")\n",
    "        return np.array([]), []\n",
    "    \n",
    "    items_in_folder = os.listdir(event_folder_path)\n",
    "    print(f\"üìÇ Found {len(items_in_folder)} items in folder\")\n",
    "    \n",
    "    # Debug: show what's in the folder\n",
    "    image_files = [f for f in items_in_folder if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp'))]\n",
    "    subdirs = [f for f in items_in_folder if os.path.isdir(os.path.join(event_folder_path, f))]\n",
    "    \n",
    "    print(f\"   üì∏ Image files: {len(image_files)}\")\n",
    "    print(f\"   üìÅ Subdirectories: {len(subdirs)}\")\n",
    "    \n",
    "    if len(image_files) > 0:\n",
    "        print(f\"   üìù Sample images: {image_files[:3]}{'...' if len(image_files) > 3 else ''}\")\n",
    "    if len(subdirs) > 0:\n",
    "        print(f\"   üìù Sample subdirs: {subdirs[:3]}{'...' if len(subdirs) > 3 else ''}\")\n",
    "    \n",
    "    has_subdirs = len(subdirs) > 0\n",
    "    has_images = len(image_files) > 0\n",
    "    \n",
    "    # Use single directory approach if we have images directly in the folder\n",
    "    if has_images:\n",
    "        # Single directory with images directly inside\n",
    "        print(\"üìÅ Using single directory processing mode\")\n",
    "        total_photos = len(image_files)\n",
    "        \n",
    "        with tqdm(total=total_photos, desc=f\"Processing Images\") as pbar:\n",
    "            for img_name in image_files:\n",
    "                img_path = os.path.join(event_folder_path, img_name)\n",
    "                try:\n",
    "                    result = DeepFace.represent(\n",
    "                        img_path=img_path,\n",
    "                        model_name=\"Facenet512\",\n",
    "                        detector_backend=\"retinaface\",\n",
    "                        align=True,\n",
    "                        enforce_detection=False,\n",
    "                    )\n",
    "                    for face_idx, face_data in enumerate(result):\n",
    "                        face_vector = face_data['embedding']\n",
    "                        facial_area = face_data['facial_area']\n",
    "                        x, y, w, h = facial_area['x'], facial_area['y'], facial_area['w'], facial_area['h']\n",
    "                        face_convidence = face_data['face_confidence']\n",
    "                        # Get original image dimensions\n",
    "                        img = Image.open(img_path)\n",
    "                        img_width, img_height = img.size\n",
    "                        \n",
    "                        # Check if face size matches full image size\n",
    "                        if w >= 800 or h >= 800:\n",
    "                            print(f\"‚ö†Ô∏è Face size matches full image in {img_path} face {face_idx}, skipping\")\n",
    "                            continue\n",
    "                        \n",
    "                        # Also keep min size check\n",
    "                        if w < min_face_size or h < min_face_size:\n",
    "                            print(f\"‚ö†Ô∏è Face too small in {img_path} face {face_idx}, skipping\") \n",
    "                            continue\n",
    "                        \n",
    "                        padding_factor = 0.3\n",
    "                        padding_x = int(w * padding_factor)\n",
    "                        padding_y = int(h * padding_factor)\n",
    "                        \n",
    "                        # Calculate new coordinates with padding\n",
    "                        new_x = max(0, x - padding_x)\n",
    "                        new_y = max(0, y - padding_y)\n",
    "                        new_right = min(img_width, x + w + padding_x)\n",
    "                        new_bottom = min(img_height, y + h + padding_y)\n",
    "                        new_w = new_right - new_x\n",
    "                        new_h = new_bottom - new_y\n",
    "                        # Crop face\n",
    "                        img = Image.open(img_path).convert(\"RGB\")\n",
    "                        cropped_img = img.crop((new_x, new_y, new_x + new_w, new_y + new_h))\n",
    "\n",
    "                        # Save cropped face with unique name\n",
    "                        cropped_name = f\"{os.path.splitext(img_name)[0]}_f-{face_idx}.jpg\"\n",
    "                        cropped_path = os.path.join(cropped_dir, cropped_name)\n",
    "                        cropped_img.save(cropped_path)\n",
    "\n",
    "                        faces.append(face_vector)\n",
    "                        metadata.append({\n",
    "                            \"foto_id\": cropped_path,  # Link ke cropped face\n",
    "                            \"album\": {\n",
    "                                \"id\": \"main\",\n",
    "                                \"name\": \"main\",\n",
    "                                \"event\": {\n",
    "                                    \"id\": event_id,\n",
    "                                    \"name\": event_id\n",
    "                                }\n",
    "                            },\n",
    "                            \"embedding\": face_vector,\n",
    "                            \"cluster_id\": None,\n",
    "                            \"path\": img_path,          # Link ke original image\n",
    "                            \"facial_area\": facial_area,\n",
    "                            \"face_confidence\": face_convidence\n",
    "                        })\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {img_path}: {e}\")\n",
    "                pbar.update(1)\n",
    "    elif has_subdirs:\n",
    "        # Nested directory structure (original logic)\n",
    "        print(\"üìÅ Using nested album processing mode\")\n",
    "        total_photos = sum(len(os.listdir(os.path.join(event_folder_path, album))) \n",
    "                           for album in os.listdir(event_folder_path) \n",
    "                           if os.path.isdir(os.path.join(event_folder_path, album)))\n",
    "\n",
    "        with tqdm(total=total_photos, desc=f\"Processing Event {event_id}\") as pbar:\n",
    "            for album_name in os.listdir(event_folder_path):\n",
    "                album_path = os.path.join(event_folder_path, album_name)\n",
    "                if os.path.isdir(album_path):\n",
    "                    album_id = album_name\n",
    "                    for img_name in os.listdir(album_path):\n",
    "                        img_path = os.path.join(album_path, img_name)\n",
    "                        if img_name.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp')):\n",
    "                            try:\n",
    "                                result = DeepFace.represent(\n",
    "                                    img_path=img_path,\n",
    "                                    model_name=\"Facenet512\",\n",
    "                                    detector_backend=\"retinaface\",\n",
    "                                    align=True,\n",
    "                                    enforce_detection=False,\n",
    "                                )\n",
    "                                for face_idx, face_data in enumerate(result):\n",
    "                                    face_vector = face_data['embedding']\n",
    "                                    facial_area = face_data['facial_area']\n",
    "                                    x, y, w, h = facial_area['x'], facial_area['y'], facial_area['w'], facial_area['h']\n",
    "                                    face_convidence = face_data['face_confidence']\n",
    "                                    # Get original image dimensions\n",
    "                                    img = Image.open(img_path)\n",
    "                                    img_width, img_height = img.size\n",
    "                                    \n",
    "                                    # Check if face size matches full image size\n",
    "                                    if w >= 800 or h >= 800:\n",
    "                                        print(f\"‚ö†Ô∏è Face size matches full image in {img_path} face {face_idx}, skipping\")\n",
    "                                        continue\n",
    "                                    \n",
    "                                    # Also keep min size check\n",
    "                                    if w < min_face_size or h < min_face_size:\n",
    "                                        print(f\"‚ö†Ô∏è Face too small in {img_path} face {face_idx}, skipping\") \n",
    "                                        continue\n",
    "                                    \n",
    "                                    padding_factor = 0.3\n",
    "                                    padding_x = int(w * padding_factor)\n",
    "                                    padding_y = int(h * padding_factor)\n",
    "                                    \n",
    "                                    # Calculate new coordinates with padding\n",
    "                                    new_x = max(0, x - padding_x)\n",
    "                                    new_y = max(0, y - padding_y)\n",
    "                                    new_right = min(img_width, x + w + padding_x)\n",
    "                                    new_bottom = min(img_height, y + h + padding_y)\n",
    "                                    new_w = new_right - new_x\n",
    "                                    new_h = new_bottom - new_y\n",
    "                                    # Crop face\n",
    "                                    img = Image.open(img_path).convert(\"RGB\")\n",
    "                                    cropped_img = img.crop((new_x, new_y, new_x + new_w, new_y + new_h))\n",
    "\n",
    "                                    # Save cropped face with unique name\n",
    "                                    cropped_name = f\"{os.path.splitext(img_name)[0]}_f-{face_idx}.jpg\"\n",
    "                                    cropped_path = os.path.join(cropped_dir, cropped_name)\n",
    "                                    cropped_img.save(cropped_path)\n",
    "\n",
    "                                    faces.append(face_vector)\n",
    "                                    metadata.append({\n",
    "                                        \"foto_id\": cropped_path,  # Link ke cropped face\n",
    "                                        \"album\": {\n",
    "                                            \"id\": album_id,\n",
    "                                            \"name\": album_name,\n",
    "                                            \"event\": {\n",
    "                                                \"id\": event_id,\n",
    "                                                \"name\": event_id\n",
    "                                            }\n",
    "                                        },\n",
    "                                        \"embedding\": face_vector,\n",
    "                                        \"cluster_id\": None,\n",
    "                                        \"path\": img_path,          # Link ke original image\n",
    "                                        \"facial_area\": facial_area,\n",
    "                                        \"face_confidence\": face_convidence\n",
    "                                    })\n",
    "                            except Exception as e:\n",
    "                                print(f\"Error processing {img_path}: {e}\")\n",
    "                        pbar.update(1)\n",
    "    else:\n",
    "        print(\"‚ùå No images or subdirectories found in the source folder!\")\n",
    "        print(f\"   Please check if '{event_folder_path}' contains image files\")\n",
    "        print(f\"   Supported formats: .jpg, .jpeg, .png, .bmp\")\n",
    "        return np.array([]), []\n",
    "\n",
    "    # Save extracted faces to cache with validation info\n",
    "    print(f\"\\nüíæ Saving face data to cache...\")\n",
    "    try:\n",
    "        faces_array = np.array(faces)\n",
    "        \n",
    "        # Save faces and metadata\n",
    "        np.save(faces_path, faces_array)\n",
    "        with open(metadata_path, \"wb\") as f:\n",
    "            pickle.dump(metadata, f)\n",
    "        \n",
    "        # Save cache validation info\n",
    "        cache_info = {\n",
    "            'created_time': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'num_faces': len(faces),\n",
    "            'num_metadata': len(metadata),\n",
    "            'folder_stats': get_folder_stats(event_folder_path),\n",
    "            'extraction_params': {\n",
    "                'min_face_size': min_face_size,\n",
    "                'model_name': \"Facenet512\",\n",
    "                'detector_backend': \"retinaface\"\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        with open(cache_info_path, \"wb\") as f:\n",
    "            pickle.dump(cache_info, f)\n",
    "        \n",
    "        print(f\"‚úÖ Cached {len(faces)} face embeddings\")\n",
    "        print(f\"‚úÖ Cached {len(metadata)} metadata entries\") \n",
    "        print(f\"‚úÖ Cache validation info saved\")\n",
    "        print(f\"üìÅ Cache location: {cache_dir}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Error saving cache: {e}\")\n",
    "        print(\"Face extraction completed but cache may be incomplete\")\n",
    "\n",
    "    return faces_array, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bfe6fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ FACE CLUSTERING WITH INTELLIGENT CACHING\n",
      "==================================================\n",
      "üîç CACHE STATUS REPORT\n",
      "========================================\n",
      "‚ùå Cache directory does not exist\n",
      "\n",
      "üîÑ FACE EXTRACTION/LOADING:\n",
      "------------------------------\n",
      "Processing event folder: Hi_concer\n",
      "üìÇ Found 232 items in folder\n",
      "   üì∏ Image files: 232\n",
      "   üìÅ Subdirectories: 0\n",
      "   üìù Sample images: ['Hiconcer_1.jpg', 'Hiconcer_10.jpg', 'Hiconcer_100.jpg']...\n",
      "üìÅ Using single directory processing mode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images:   2%|‚ñè         | 4/232 [00:13<12:48,  3.37s/it]"
     ]
    }
   ],
   "source": [
    "FORCE_REEXTRACT = False  # Set to True to ignore cache and re-extract all faces\n",
    "SOURCE_FOLDER = \"Hi_concer\"  # Folder containing images to process\n",
    "CACHE_DIR = \"coba_cache\"  # Cache directory\n",
    "\n",
    "print(\"üöÄ FACE CLUSTERING WITH INTELLIGENT CACHING\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Show current cache status\n",
    "show_cache_status(CACHE_DIR)\n",
    "\n",
    "# Check if you want to force re-extraction (useful for testing different parameters)\n",
    "if FORCE_REEXTRACT:\n",
    "    print(\"‚ö†Ô∏è  FORCE_REEXTRACT=True: Will ignore cache and re-extract all faces\")\n",
    "\n",
    "print(f\"\\nüîÑ FACE EXTRACTION/LOADING:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Extract or load faces using the improved caching system\n",
    "faces, foto_data = extract_faces_from_event_folder(\n",
    "    event_folder_path=SOURCE_FOLDER,\n",
    "    cache_dir=CACHE_DIR,\n",
    "    force_reextract=FORCE_REEXTRACT\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä FACE DATA SUMMARY:\")\n",
    "print(f\"   ‚Ä¢ Total faces: {len(faces)}\")\n",
    "if len(faces) > 0:\n",
    "    print(f\"   ‚Ä¢ Face dimensions: {faces.shape[1]}D\")\n",
    "print(f\"   ‚Ä¢ Metadata entries: {len(foto_data)}\")\n",
    "\n",
    "# Check if we have faces to cluster\n",
    "if len(faces) == 0:\n",
    "    print(\"‚ùå No faces found to cluster! Check your source folder and try re-extracting.\")\n",
    "    print(\"üí° Set FORCE_REEXTRACT=True to re-process images\")\n",
    "    exit()\n",
    "\n",
    "print(f\"   ‚Ä¢ Ready for clustering! üéØ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86c78e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "foto_data[2][\"face_confidence\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
